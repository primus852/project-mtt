In the last decade, the development of gesture recognition technology has led to a breakthrough in SLA translators. There are two main approaches to sign language recognition: sensor-based or vision-based \cite{Cheok2019ARO}. The major difference between sensor-based and vision-based approaches is in the data acquisition phase. Sensor-based approaches utilize sensor instruments such as sensory gloves to capture sign language, but such equipment was too complex and expensive to be widely actual used. On the other hand, vision-based approaches don't require complex facilities to acquire data, acquiring images or videos of the sign language through a camera. For example, D., Cao et al.\cite{7301347} in 2015 developed sign language recognition by adapting Microsoft Kinect technology and used Random forest to successfully recognized static 24 American SLA with above 90\% accuracy. Furthermore, A., Joshi et al.\cite{8088212} presented a real-time automated American SLA translator that translates American SLA to English text by applying edge detection and cross-correlation methodologies, resulting in 94.23\% accuracy for alphabets. The Convolutional neural network (CNN) has become a common method applied in image recognition and classification in recent years. M., Taskiran et al.\cite{8441304} designed a real-time sign language system with the implementation of feature extraction and classifier based on a CNN structure, resulting in 98.05\% accuracy.