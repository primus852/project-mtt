In the last decade, the development of gesture recognition technology has led to a breakthrough in SLA translators. There are two main approaches to sign language recognition: sensor-based or vision-based\cite{Cheok2019ARO}. The major difference between sensor-based and vision-based approaches is in the data acquisition phase. Sensor-based approaches utilize sensor instruments such as sensory gloves to capture sign language, but such equipment was too complex and expensive to be widely used. On the other hand, vision-based approaches don't require complex facilities to acquire data, acquiring images or videos of the sign language through a camera. For example, D., Cao et al.\cite{7301347} in 2015 developed sign language recognition by adapting Microsoft Kinect technology and used Random forest to successfully recognized static 24 American SLA with above 90\% accuracy. Furthermore, A., Joshi et al.\cite{8088212} presented a real-time automated American SLA translator that translates American SLA to English text by applying edge detection and cross-correlation methodologies, resulting in 94.23\% accuracy for alphabets. The Convolutional neural network (CNN) has become a common method applied in image recognition and classification in recent years. M., Taskiran et al.\cite{8441304} designed a real-time sign language system with the implementation of feature extraction and classifier based on a CNN structure, resulting in 98.05\% accuracy.