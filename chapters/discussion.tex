The training of the long and the short list in chapter \ref{chapter_results} diminished the importance of the training time, but therefore the key factor for choosing the best model for the application became the inference time. \textit{MobileNetV2} has the best inference time of 0.03 seconds per prediction, allowing a framerate of approx. 33 fps. While this can result in a smooth video in real-time applications, many of the current webcams allow a higher framerate of up to 60 fps. For the model not to slow down the actual output, the next steps should focus on the improvement of the inference time.

We identified three major leverage factors for improving the inference time: reducing the input layers, adjust the training settings in \textit{Keras} and implement a distributed prediction with \textit{DKeras}.

\subsection{Reducing the input layers}\label{chapter_distill}
Improving the inference time of our model can be achieved by a process called \textit{Knowledge Distillation}\cite{hinton2015distilling}. Models typically use a \textit{softmax} output layer to convert the logarithmic value of the model to a probability by comparing it to the other logarithmic values. Geoffrey Hinton et al\cite{hinton2015distilling} propose a method to derive a distilled model from the original one by averaging two weighted functions. The first function uses the cross entropy with the soft targets; the second function is the cross entropy with the correct labels. Tests with various datasets showed that the speed of the distilled models increased while the accuracy decreased by 1-5\%, depending on the dataset and architecture.

The accuracy of \textit{MobilNetV2} is at 99\% (see Table \ref{tab:results:short}), even with a decreased accuracy of approx. 94\% with a distilled model, assuming an unchanged inference time of 0.03s, this leads to a wrong label every other 2.82 seconds for one frame, down from 2.97s with a 99\% accuracy. Since the actual prediction needs to be determined over a certain amount of frame, this is a non-factor. Even if the inference time does not increase by a significant margin, any improvement can lead to a better user experience and therefore should be tested.

\subsection{Adjust the precision}
Another possible improvement of the inference time is to decrease the precision of the weights. This can be achieved in two ways: pre or post training.

The \textit{Post Training Quantization}\cite{postprec}, a method implemented in \textit{Tensorflow}, converts the weights after the training from floating points of 32-bits to integers of 8-bits. This decreases the accuracy of the model, but therefore significantly improves the model size in memory, which leads to a much lower latency of the model.

Between the 32-bits and 8-bits precision, there is also the option to convert the weights to a half-precision approach of 16-bit floating points. The \textit{Mixed Precision Training} reduces the model size by 75\% but needs to be specified before the training. During training, the 16-bit weights may result in an underflow of the weight (the value becomes zero due to the gradient being too small for the half-precision), causing the model to learn incorrectly. Not all layers play the same role with the inference time. The \textit{Automatic Mixed Precision} enables the half-precision only for optimizable layers, such as convolutional layers.\cite{micikevicius2018mixed}

Both approaches may be suitable for further testing, as in chapter \ref{chapter_distill} mentioned before, a small loss in accuracy won't be noticeable for the user if implemented correctly.

\subsection{Implement dKeras}
Distributed Keras Engine (dKeras)\cite{dkeras} is an open-source project that focuses solely on improving the inference time of models used in production environments without the need for special hardware. The core of the project is to run the predictions in parallel and therefore improve the inference time significantly without needing to re-train or convert the used model. The documentation shows that only two lines of code need to be replaced in order to implement the optimized predict method. According to the benchmark and depending on the architecture of the used model and the available dKeras workers, the inference time improves approx. 94-97\% without decreasing the accuracy. 

Using dKeras for a fully trained model could decrease the importance of the inference for all models. A 94\% improved inference time for the \textit{DenseNet201} (the slowest of the models, see Fig. \ref{fig:result:inference}) leads to an inference time of approx. 0.0084 seconds, a framerate of 2738 fps. At this point, no consumer hardware has the capability of outputting that framerate.

Further improvements should focus on the implementation of dKeras.