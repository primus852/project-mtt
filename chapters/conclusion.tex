This scientific work proposed an applied review of multiple state-of-art deep learning models to elaborate their usability for sign language translation. The specified problem of not having intense research in the field of translating letters from one sign language to another was the main motivation. The goal was to compare multiple models with regard to accuracy such as latency to make an educated guess of which of the proposed models is the most suited one for a user facing application. To validate which model to choose, a 87.000 image big dataset with American sign language letters was used as training data. Starting with a long list of 27 pre-trained models from the Keras framework a short list of only the 6 best performing models was chosen to proceed with. The models were trained by applying transfer learning to maintain the previously learned weights. All models were compared based on their accuracy and prediction time such as the number of frames per second which they can process. It was shown that the accuracy of all models was quite similar which led to the fact that no model could have been proposed solely based on this indicator. However due to the different model architectures the research showed significant difference in the prediction time such as number per frames which were processed. MobileNetV1 resulted as preferred model from this comparison as the model accuracy such as its prediction speed were superior compared to the other architectures.
Nevertheless the current workflow is only applicable for languages which share the same letters as the predicted output is only mapped to the other language's letter. Further work has to include a more sophisticated word and letter embedding for the actual translation after a correct prediction.
Also the actual performance of an implemented infrastructure has to be tested as all experiments were executed in-vitro.