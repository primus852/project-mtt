The World Health Organization (WHO) projected that nearly 2.5 billion people in the world have hearing loss by 2050 and emphasized that sign language-related applications are essential tools for deaf people \cite{WHO2050}.

Instead of oral language using speaking and listening, Sign language is a form of visual communication through gestures, body movements and facial expressions. Sign language can be used for different purposes in different situations, but it is primarily designed for communication with the deaf. As it is more visually accessible to the deaf, sign language is a natural method of communication for the deaf people and can express a wide range of meanings in the same way as spoken language; for example, American Sign Language is used by deaf people in the United States and partial provinces of Canada, British Sign Language is used by deaf people in the UK, French Sign Language in France, Japanese Sign Language in Japan and Chinese Sign Language in China.

Although there is no unified international sign language yet, the major sign language systems such as the USA, British, France and China have developed fingerspelling, also called "Sign Language Alphabet" (SLA). SLA represents the 26 common alphabets, A to Z, and local special letters using only hand gestures, mostly in difficult words such as personal names, place names, technical terms, etc. Additionally, it can also be used to facilitate easier communication when encountering issues with sign language dialects. 

In the past decade, deep learning has shown excellent performance in image recognition, enabling breakthroughs in sign language recognition technology. Many studies have used neural networks to convert static SLA signs into text or speech, using images of hand gestures as input, and the main approach includes data acquisition technique, static signs, classification with over 90\% accuracy of recognition \cite{wadhawan2021sign}. Moreover, Transfer Learning is one of the major milestones in Deep Learning for Object Detection. With transfer learning, pre-trained models have already been trained on a different task; therefore, it is a short-cut that re-uses the pre-trained model such as its trained weights for a shorter training process. This project aims to utilize pre-trained models from transfer learning with Object Detection in Keras and to apply static hand gesture images into different pre-trained model architecture to present a development of an SLA translator, which translates American SLA to Turkish SLA, thereby helping deaf people to communicate with each other without learning a new sign language. The remainder of this paper will include related work in the sign language recognition research area, the problem statement found in the research, the objective of the project, and the methodology applied in the project. After evaluating the top accuracy rate from each architecture from all the available pre-trained models in the Kera transfer learning with Object Detection library, six pre-trained models, \textit{VGG16}, \textit{ResNet50V2}, \textit{MobileNet}, \textit{MobileNetV2}, \textit{DenseNet201}, and \textit{Xception}, are selected into the short list and trained as predictive models with a full training dataset. The results of relevant accuracy, training duration, and inference time were evaluated as final metrics to determine the most suitable model for the use case in real-time translation. In the end, the most suitable model is used to predict the Turkish SLA images from the ASL images using the test dataset.